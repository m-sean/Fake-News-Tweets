{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 14406\n",
      "Total unique (text): 14000\n",
      "\n",
      "Tweets collected between:\n",
      "Sat Dec 08 16:23:07 +0000 2018\n",
      "Wed Dec 05 19:17:44 +0000 2018\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "with open(\"fake-news-Dec-8th.json\", \"r\") as source:\n",
    "    alltweets = json.load(source)\n",
    "\n",
    "unique_tweets = set()\n",
    "for tweet in alltweets:\n",
    "    unique_tweets.update([tweet['full_text']])\n",
    "\n",
    "print(f\"Total tweets: {len(alltweets)}\")\n",
    "print(f\"Total unique (text): {len(unique_tweets)}\")\n",
    "\n",
    "ids =[]\n",
    "for tweet in alltweets:\n",
    "    ids.append(tweet['id'])  \n",
    "    \n",
    "ids.sort(reverse=True)\n",
    "first=ids[0]\n",
    "last=ids[-1]\n",
    "\n",
    "print(\"\\nTweets collected between:\")\n",
    "for tweet in alltweets:\n",
    "    if tweet['id'] == first:\n",
    "        print(tweet['created_at'])\n",
    "    if tweet['id'] == last: \n",
    "        print(tweet['created_at'])\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tweet_tokenize(tweet: dict,*,casefold=False) -> List[str]:\n",
    "    pattern = \"\"\"(?x)                  # VERBOSE\n",
    "        (?:[A-Za-z]\\.)+                # abbreviations\n",
    "        | \\w+['â€™]\\w\\w?                 # contractions\n",
    "        | \\#\\w+                        # hashtags\n",
    "        | \\@\\w+                        # mentions\n",
    "        | https?://\\w+\\.\\w+\\.?(?:/\\w+) # links\n",
    "        | \\w+(?:-\\w+)*                 # hyphenated words/names\n",
    "        | \\$?\\d+(?:\\.\\d+)?%?           # currency or percentages\n",
    "        | &amp;                        # &\n",
    "        \"\"\"\n",
    "    if casefold: text = tweet['full_text'].casefold()\n",
    "    else: text = tweet['full_text']\n",
    "    return nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "def ngrams(symbols: list, n=3):\n",
    "    if len(symbols) < n:\n",
    "        return\n",
    "    prev_context = symbols[:n - 1]\n",
    "    for i in range(len(symbols) - n + 1):\n",
    "        yield tuple(symbols[i:i + n])\n",
    "        \n",
    "def has_hash(tweets: List[dict]) -> List[dict]:\n",
    "    # Yields all tweets that contain hashtag(s)\n",
    "    for tweet in tweets:\n",
    "        if tweet['entities']['hashtags']: yield tweet\n",
    "            \n",
    "def has_url(tweets: List[dict]) -> List[dict]:\n",
    "    # Yields all tweets that contain URL(s)\n",
    "    # Urls indicate quoted tweets link/media sharing, etc.\n",
    "    for tweet in tweets:\n",
    "        if tweet['entities']['urls']: yield tweet\n",
    "\n",
    "def has_quote(tweets: List[dict]) -> List[dict]:\n",
    "    # Yields tweets that quote another tweet\n",
    "    for tweet in tweets:\n",
    "        if tweet['is_quote_status']: yield tweet\n",
    "            \n",
    "def yield_quoted(tweets: List[dict]) -> dict:\n",
    "    # Yields tweets quoted by another user\n",
    "    # Identifies tweets marked as quoting but don't contain quoted content\n",
    "    for tweet in has_quote(tweets):\n",
    "        try: yield tweet['quoted_status']\n",
    "        except KeyError: print(f\"id: {tweet['id']} has no quoted tweet.\")\n",
    "\n",
    "def get_quoted(tweet):\n",
    "    return tweet['quoted_status']\n",
    "\n",
    "def tweet_hashtags(tweet: dict,*,casefold=True) -> str:\n",
    "    # Yields hashtag(s) of a tweet\n",
    "    taglst = tweet['entities']['hashtags']\n",
    "    for tag in taglst:\n",
    "        if casefold: yield tag['text'].casefold()\n",
    "        else: yield tag['text']\n",
    "\n",
    "def tweet_urls(tweet: dict,*,casefold=False) -> str:\n",
    "    # Yields non-status URL(s) embedded in a given tweet\n",
    "    # some urls are case-sensitive\n",
    "    statusfilter='twitter.com/\\w+?/status/'\n",
    "    in_urlst = tweet[\"entities\"][\"urls\"]\n",
    "    out_urlst = []\n",
    "    for url_dict in in_urlst:\n",
    "        if not re.findall(statusfilter, url_dict['expanded_url']):\n",
    "            out_urlst.append(url_dict)\n",
    "    for url in out_urlst:\n",
    "        if casefold: yield url['expanded_url'].casefold()\n",
    "        else: yield url['expanded_url']\n",
    "    \n",
    "def get_screenname(tweet: dict,*,casefold=True) -> str:\n",
    "    if casefold: return tweet['user']['screen_name'].casefold()\n",
    "    else: return tweet['user']['screen_name']\n",
    "\n",
    "def search_keys(target_query: str,target_dict: dict) -> str:\n",
    "    # Yield keys to access tweets by hashtag, url or quoted user\n",
    "    for key in target_dict.keys():\n",
    "        if target_query in key: yield key   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then avergage tweet length (without urls) is *21.57* tokens.\n",
      "*2626* tweets are <= 10 chars and begin or end with 'fake news'.\n",
      "*1294* involve some yelling.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Casefold and tokenize all tweets, and get rid of urls\n",
    "tokenized_tweets = []\n",
    "for tweet in alltweets:\n",
    "    tokens = []\n",
    "    for token in tweet_tokenize(tweet,casefold=True):\n",
    "        if not token.startswith('http'): tokens.append(token)\n",
    "    tokenized_tweets.append(tokens)\n",
    "\n",
    "# Calculate avg length of tweets\n",
    "all_lens=0\n",
    "for tweet in tokenized_tweets:\n",
    "    all_lens+=len(tweet)\n",
    "avg_len = all_lens/len(tokenized_tweets)\n",
    "print(f\"Then avergage tweet length (without urls) is *{round(avg_len,2)}* tokens.\")\n",
    "\n",
    "# Complie all short tweets that begin or end with 'fake news'\n",
    "shortnsweet = []\n",
    "for tweet in tokenized_tweets:\n",
    "    if len(tweet) <= 10 \\\n",
    "    and (tweet[0:2]==['fake','news'] or tweet[-2:]==['fake','news']):\n",
    "        shortnsweet.append(tweet)\n",
    "print(f\"*{len(shortnsweet)}* tweets are <= 10 chars and begin or end with 'fake news'.\")  \n",
    "\n",
    "# Find tweets that are at least 30% in ALL CAPS\n",
    "yelling = []\n",
    "for tweet in alltweets:\n",
    "    tokens = tweet_tokenize(tweet,casefold=False)\n",
    "    yells=0\n",
    "    for token in tokens:\n",
    "        if token.isupper(): yells+=1\n",
    "    if yells/len(tokens) >= 0.3: yelling.append(tweet)\n",
    "        \n",
    "print(f\"*{len(yelling)}* involve some yelling.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*2349* tweets contain hashtags.\n",
      "*9936* tweets contain urls.\n",
      ">> *5654* tweets are marked as quoting another user.\n",
      ">> *4282* tweets shared at least one link.\n",
      ">> *5654* quoted tweets are accesible via the API.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Collect all tweets with hashtags\n",
    "has_tags = []\n",
    "for tweet in has_hash(alltweets):\n",
    "    has_tags.append(tweet)\n",
    "print(f\"*{len(has_tags)}* tweets contain hashtags.\")\n",
    "    \n",
    "# Collect all tweets with urls\n",
    "# Urls indicate quoted tweets *and* links\n",
    "has_urls=[]\n",
    "for tweet in has_url(alltweets):\n",
    "    has_urls.append(tweet)\n",
    "print(f\"*{len(has_urls)}* tweets contain urls.\")\n",
    "\n",
    "# Collect only the tweets marked as quoting another user\n",
    "are_quoting=[]\n",
    "for tweet in has_quote(alltweets):\n",
    "    are_quoting.append(tweet)\n",
    "print(f\">> *{len(are_quoting)}* tweets are marked as quoting another user.\")\n",
    "\n",
    "# Collect all other tweets that shared a link\n",
    "not_quoting=[]\n",
    "for tweet in has_urls:\n",
    "    if tweet not in are_quoting:\n",
    "        not_quoting.append(tweet)\n",
    "print(f\">> *{len(not_quoting)}* tweets shared at least one link.\")\n",
    "\n",
    "# Collect the embedded quoted tweets\n",
    "is_quoted_tweet=[]\n",
    "for tweet in yield_quoted(alltweets):\n",
    "    is_quoted_tweet.append(tweet)\n",
    "print(f\">> *{len(is_quoted_tweet)}* quoted tweets are accesible via the API.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> *5654* tweets are now marked as quoting another user.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# clean up tweets that are quote-marked w/o an embedded tweet\n",
    "bad = {}\n",
    "for tweet in are_quoting:\n",
    "    if tweet['id'] in bad:\n",
    "        tweet['is_quote_status'] = False\n",
    "\n",
    "are_quoting=[]\n",
    "for tweet in has_quote(alltweets):\n",
    "    are_quoting.append(tweet)\n",
    "print(f\">> *{len(are_quoting)}* tweets are now marked as quoting another user.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 quoted users:\n",
      "---\n",
      "1047 tweets quoted @realdonaldtrump.\n",
      "---\n",
      "81 tweets quoted @civmilair.\n",
      "---\n",
      "80 tweets quoted @joshuamckerrow.\n",
      "---\n",
      "70 tweets quoted @donlemon.\n",
      "---\n",
      "47 tweets quoted @cnn.\n",
      "---\n",
      "46 tweets quoted @brianstelter.\n",
      "---\n",
      "31 tweets quoted @dbongino.\n",
      "---\n",
      "30 tweets quoted @washingtonpost.\n",
      "---\n",
      "27 tweets quoted @ap.\n",
      "---\n",
      "26 tweets quoted @thehill.\n",
      "---\n",
      "25 tweets quoted @cnnbrk.\n",
      "---\n",
      "24 tweets quoted @breitbartnews.\n",
      "---\n",
      "23 tweets quoted @krassenstein.\n",
      "---\n",
      "23 tweets quoted @danrather.\n",
      "---\n",
      "19 tweets quoted @cnnpr.\n",
      "---\n",
      "18 tweets quoted @nbcnews.\n",
      "---\n",
      "17 tweets quoted @realjameswoods.\n",
      "---\n",
      "16 tweets quoted @mflynnjr.\n",
      "---\n",
      "15 tweets quoted @lisamei62.\n",
      "---\n",
      "14 tweets quoted @msnbc.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Collect 'quoted' tweet entities/users for comparison\n",
    "quoted_users = set()\n",
    "for tweet in are_quoting:\n",
    "    quoted_users.update([get_screenname(get_quoted(tweet))])\n",
    "    \n",
    "# Map tweets by the quoted entity:\n",
    "by_quoted_user = nltk.defaultdict(list)\n",
    "for tweet in are_quoting:\n",
    "    q = get_quoted(tweet)\n",
    "    by_quoted_user[get_screenname(q)].append(tweet)\n",
    "\n",
    "# Make sure all quoted users are accounted for\n",
    "assert len(quoted_users) == len(by_quoted_user)\n",
    "\n",
    "# Calculate the number of tweets quoting each entity\n",
    "tweets_per_quoted=[]\n",
    "for key in by_quoted_user.keys():\n",
    "    tweets_per_quoted.append((key, len(by_quoted_user[key])))\n",
    "    \n",
    "tweets_per_quoted.sort(key=lambda x: x[1], reverse=True)\n",
    "top_20_quoted = tweets_per_quoted[0:20]\n",
    "\n",
    "print(\"Top 20 quoted users:\")\n",
    "for ent, num in top_20_quoted:\n",
    "    print('---')\n",
    "    print(f\"{num} tweets quoted @{ent}.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags associated with @civmilair:\n",
      "#qanon76\n",
      "#q\n",
      "#qarmy\n",
      "#fakenews\n",
      "#greatawakening\n",
      "#wwg1gwa\n",
      "#msnbc\n",
      "#qanon\n",
      "#maga2kag\n",
      "#qposts\n",
      "#qanonposts\n",
      "#patriotsfight\n",
      "#declasfisa\n",
      "#lyingnews\n",
      "#therainmakers\n",
      "#asktheq\n",
      "#wwg1wgaworldwide\n",
      "#maga\n",
      "#thestormishere\n",
      "#panicindc\n",
      "#thegreatawakening\n",
      "#shadowbanned\n",
      "#wrwy\n",
      "#qanon8chan\n",
      "#qdrop\n",
      "#nbc\n",
      "#qotus\n",
      "#qalert\n",
      "#trump\n",
      "#kag\n",
      "#trusttheplan\n",
      "#controlledmedia\n",
      "#cnn\n",
      "#covfefe\n",
      "#news\n",
      "#doitq\n",
      "#bebest\n",
      "#wwg1wga\n",
      "#newq\n"
     ]
    }
   ],
   "source": [
    "# Map hashtags associated with the quoted entities\n",
    "tags_with_quoted = nltk.defaultdict(set)\n",
    "for key in by_quoted_user.keys():\n",
    "    for tweet in has_hash(by_quoted_user[key]):\n",
    "        tags_with_quoted[key].update(tweet_hashtags(tweet))\n",
    "\n",
    "print(\"Tags associated with @civmilair:\")\n",
    "print(\"#\"+\"\\n#\".join(tags_with_quoted['civmilair']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@monsterjohn6 tweeted 135 times.\n",
      "---\n",
      "@allthenewsbot tweeted 62 times.\n",
      "---\n",
      "@dealriot tweeted 40 times.\n",
      "---\n",
      "@ohaifakenewsbot tweeted 38 times.\n",
      "---\n",
      "@posts_from_asia tweeted 28 times.\n",
      "---\n",
      "@realdonaldtwump tweeted 24 times.\n",
      "---\n",
      "@kickingfacts tweeted 20 times.\n",
      "---\n",
      "@leadstoriescom tweeted 20 times.\n",
      "---\n",
      "@santiago_compos tweeted 20 times.\n",
      "---\n",
      "@baby_talkbot tweeted 18 times.\n",
      "---\n",
      "@carminezozzora tweeted 18 times.\n",
      "---\n",
      "@nigeriantribune tweeted 17 times.\n",
      "---\n",
      "@algorithmtrump tweeted 16 times.\n",
      "---\n",
      "@trumpstwo tweeted 15 times.\n",
      "---\n",
      "@dykbot tweeted 13 times.\n",
      "---\n",
      "@mgoriganti tweeted 13 times.\n",
      "---\n",
      "@alandukenews tweeted 12 times.\n",
      "---\n",
      "@mschenk tweeted 12 times.\n",
      "---\n",
      "@kuj_kenya tweeted 11 times.\n",
      "---\n",
      "@botdonaldtrump_ tweeted 11 times.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Count frequecy of users among all tweets\n",
    "most_active = nltk.FreqDist()\n",
    "for tweet in alltweets:\n",
    "    most_active.update([get_screenname(tweet)])\n",
    "\n",
    "allusers_sorted=most_active.most_common()\n",
    "\n",
    "# These are mostly bots\n",
    "for ent, num in allusers_sorted[0:20]:\n",
    "    print(f\"@{ent} tweeted {num} times.\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#fakenews was tweeted 285 times.\n",
      "---\n",
      "#maga was tweeted 177 times.\n",
      "---\n",
      "#qanon was tweeted 158 times.\n",
      "---\n",
      "#fake was tweeted 90 times.\n",
      "---\n",
      "#news was tweeted 89 times.\n",
      "---\n",
      "#wwg1wga was tweeted 88 times.\n",
      "---\n",
      "#trump was tweeted 79 times.\n",
      "---\n",
      "#cnn was tweeted 73 times.\n",
      "---\n",
      "#factsmatter was tweeted 60 times.\n",
      "---\n",
      "#reuters was tweeted 39 times.\n",
      "---\n",
      "#wsj was tweeted 38 times.\n",
      "---\n",
      "#picture was tweeted 37 times.\n",
      "---\n",
      "#kara was tweeted 37 times.\n",
      "---\n",
      "#psy was tweeted 37 times.\n",
      "---\n",
      "#bigbang was tweeted 37 times.\n",
      "---\n",
      "#2ne1 was tweeted 37 times.\n",
      "---\n",
      "#infinite was tweeted 37 times.\n",
      "---\n",
      "#kpop was tweeted 37 times.\n",
      "---\n",
      "#kpoptv was tweeted 37 times.\n",
      "---\n",
      "#friends was tweeted 37 times.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of times each tag appears\n",
    "tagfreq = nltk.FreqDist()\n",
    "for tweet in has_hash(alltweets):\n",
    "    tagfreq.update(tweet_hashtags(tweet))\n",
    "    \n",
    "alltags_sorted=tagfreq.most_common()\n",
    "\n",
    "# Map tweets by the hashtags they contain\n",
    "tweets_by_tag = nltk.defaultdict(list)\n",
    "for tweet in has_hash(alltweets):\n",
    "    for tag in tweet_hashtags(tweet):\n",
    "        tweets_by_tag[tag].append(tweet)\n",
    "        \n",
    "assert len(alltags_sorted) == len(tweets_by_tag)\n",
    "\n",
    "for tag, num in alltags_sorted[0:20]:\n",
    "    print(f\"#{tag} was tweeted {num} times.\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Strip urls as much as possible to map tweets by their news source\n",
    "all_links = []\n",
    "for tweet in not_quoting:\n",
    "    for tweeturl in tweet_urls(tweet):\n",
    "        all_links.append(tweeturl)\n",
    "        \n",
    "url_stripped=\"\"\"(?x) \n",
    "            (?:https?://)?          # http or https protocol (non-capturing)\n",
    "            (                       # begin capture match.groups()[0]\n",
    "            (www\\.\\w+(-?\\.?\\w+)?)?  # www + host name (optional)\n",
    "            (\\w+-)?(\\w+)?           # cont'd host name with hyphen  \n",
    "            (\\.\\w+(-?\\.?\\w+)*)?     # cont'd host names w/ hypen or dot                 \n",
    "            )                       # end capture match.groups()[0]\n",
    "            \"\"\"\n",
    "\n",
    "sources = dict()\n",
    "stripped_urls = set()\n",
    "\n",
    "for url in all_links:\n",
    "    # Map stripped urls by the full url in the tweet\n",
    "    sources[url]=re.search(url_stripped, url).groups()[0]\n",
    "    # Also collect stripped urls for comparison\n",
    "    stripped_urls.update([re.search(url_stripped, url).groups()[0]])\n",
    "\n",
    "## Map tweets by stripped url\n",
    "tweets_by_source=nltk.defaultdict(list)\n",
    "for tweet in not_quoting:\n",
    "    for url in tweet_urls(tweet):\n",
    "        tweets_by_source[sources[url]].append(tweet)\n",
    "\n",
    "assert len(stripped_urls) == len(tweets_by_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "424 shared from youtu.be\n",
      "---\n",
      "183 shared from www.thegatewaypundit.com\n",
      "---\n",
      "122 shared from www.facebook.com\n",
      "---\n",
      "82 shared from www.cnn.com\n",
      "---\n",
      "76 shared from www.papermag.com\n",
      "---\n",
      "74 shared from www.theguardian.com\n",
      "---\n",
      "71 shared from www.youtube.com\n",
      "---\n",
      "70 shared from hoax-alert.leadstories.com\n",
      "---\n",
      "69 shared from www.reuters.com\n",
      "---\n",
      "65 shared from a.msn.com\n",
      "---\n",
      "62 shared from allthenews.bigcartel.com\n",
      "---\n",
      "58 shared from www.huffingtonpost.com\n",
      "---\n",
      "55 shared from www.cnbc.com\n",
      "---\n",
      "44 shared from www.rawstory.com\n",
      "---\n",
      "44 shared from apple.news\n",
      "---\n",
      "43 shared from www.westernjournal.com\n",
      "---\n",
      "43 shared from www.dailymail.co.uk\n",
      "---\n",
      "42 shared from www.foxnews.com\n",
      "---\n",
      "42 shared from www.washingtonpost.com\n",
      "---\n",
      "40 shared from deal-riot.com\n",
      "---\n",
      "---\n",
      "GatewayPundit associated tags: \n",
      "#mueller, #q, #trump, #danangdick, #aag, #spygate, #ricothedems, #arrestthemallnow, #tcot, #breaking, #fakenews, #walkaway, #wwg1wga, #blumenthal, #qanon, #maga\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of tweets containing each source (e.g. 'www.cnn.com')\n",
    "tweets_per_source = []\n",
    "for key in tweets_by_source.keys():\n",
    "    tweets_per_source.append((key,len(tweets_by_source[key])))\n",
    "    \n",
    "tweets_per_source.sort(key=lambda x: x[1], reverse=True)\n",
    "top_20_sources = tweets_per_source[0:20]\n",
    "\n",
    "# Map tags associtated with each source\n",
    "associated_tags = nltk.defaultdict(set)\n",
    "for key in tweets_by_source.keys():\n",
    "    for tweet in has_hash(tweets_by_source[key]): \n",
    "        associated_tags[key].update(tweet_hashtags(tweet))\n",
    "        \n",
    "for source,num in top_20_sources:\n",
    "    print('---')\n",
    "    print(f\"{num} shared from {source}\")\n",
    "print('---')\n",
    "print('---')\n",
    "print('GatewayPundit associated tags: ')\n",
    "print('#'+', #'.join(associated_tags['www.thegatewaypundit.com']))\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881 tweets under 5 tokens avg 37.6% title case.\n",
      "2381 tweets w/ 5-10 tokens avg 26.9% title case.\n",
      "2228 tweets w/ 10-15 tokens avg 32.0% title case.\n",
      "1802 tweets w/ 15-20 tokens avg 30.9% title case.\n",
      "2263 tweets w/ 20-30 tokens avg 19.3% title case.\n",
      "1995 tweets w/ 30-40 tokens avg 18.0% title case.\n",
      "1583 tweets w/ 40-50 tokens avg 16.2% title case.\n",
      "273 tweets over 50 tokens avg 12.1% title case.\n",
      "---\n",
      "3058 tweets are above average for their group.\n",
      "11348 tweets are about at or below average.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "to5=[]\n",
    "to10=[] \n",
    "to15=[] \n",
    "to20=[]\n",
    "to30=[] \n",
    "to40=[]\n",
    "to50=[] \n",
    "more50=[]\n",
    "divided_by_size = [to5, to10, to15, to20, to30, to40, to50, more50]\n",
    "\n",
    "tokenized = (tweet_tokenize(tweet,casefold=False) for tweet in alltweets)\n",
    "for tweet in tokenized:\n",
    "    tokens = []\n",
    "    for token in tweet:\n",
    "        if not token.startswith('http') \\\n",
    "        and not token.startswith('@') \\\n",
    "        and not token.startswith('#'): tokens.append(token)\n",
    "    if len(tokens) <= 5: to5.append(tokens)\n",
    "    elif 5 < len(tokens) <= 10: to10.append(tokens)\n",
    "    elif 10 < len(tokens) <= 15: to15.append(tokens)\n",
    "    elif 15 < len(tokens) <= 20: to20.append(tokens)\n",
    "    elif 20 < len(tokens) <= 30: to30.append(tokens)\n",
    "    elif 30 < len(tokens) <= 40: to40.append(tokens)\n",
    "    elif 40 < len(tokens) <= 50: to50.append(tokens)\n",
    "    else: more50.append(tokens)\n",
    "\n",
    "def count_titles(tokens):\n",
    "    total = 0\n",
    "    for token in tokens:\n",
    "        if token.istitle(): total+=1\n",
    "    return total\n",
    "\n",
    "individual_avgs = []\n",
    "for i in range(len(divided_by_size)):\n",
    "    t_avg = []\n",
    "    for tokens in divided_by_size[i]:\n",
    "        titles=count_titles(tokens)\n",
    "        avg_title=round(titles/len(tokens)*100)\n",
    "        t_avg.append(avg_title)\n",
    "    individual_avgs.append(t_avg)\n",
    "\n",
    "ovall_avgs=[]\n",
    "for i in range(len(individual_avgs)):\n",
    "    all_avgs = 0\n",
    "    for avg in individual_avgs[i]: all_avgs+=avg\n",
    "    ovall_avgs.append(all_avgs/len(individual_avgs[i]))\n",
    "    \n",
    "plusavg_titlecase = []\n",
    "avg_titlecase = []\n",
    "for i in range(8):\n",
    "    for index in range(len(divided_by_size[i])):\n",
    "        if individual_avgs[i][index] > round(ovall_avgs[i],1)*1.5: \n",
    "            plusavg_titlecase.append(divided_by_size[i][index])\n",
    "        else: \n",
    "            avg_titlecase.append(divided_by_size[i][index])\n",
    "            \n",
    "print(f\"{len(to5)} tweets under 5 tokens avg {round(ovall_avgs[0],1)}% title case.\")\n",
    "print(f\"{len(to10)} tweets w/ 5-10 tokens avg {round(ovall_avgs[1],1)}% title case.\")\n",
    "print(f\"{len(to15)} tweets w/ 10-15 tokens avg {round(ovall_avgs[2],1)}% title case.\")\n",
    "print(f\"{len(to20)} tweets w/ 15-20 tokens avg {round(ovall_avgs[3],1)}% title case.\")\n",
    "print(f\"{len(to30)} tweets w/ 20-30 tokens avg {round(ovall_avgs[4],1)}% title case.\")\n",
    "print(f\"{len(to40)} tweets w/ 30-40 tokens avg {round(ovall_avgs[5],1)}% title case.\")\n",
    "print(f\"{len(to50)} tweets w/ 40-50 tokens avg {round(ovall_avgs[6],1)}% title case.\")\n",
    "print(f\"{len(more50)} tweets over 50 tokens avg {round(ovall_avgs[7],1)}% title case.\")\n",
    "print('---')\n",
    "print(f\"{len(plusavg_titlecase)} tweets are above average for their group.\")\n",
    "print(f\"{len(avg_titlecase)} tweets are about at or below average.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 tweets of:\n",
      "\"Fake News\"\n",
      "---\n",
      "137 tweets of:\n",
      "\"Grassley Shuts Down Da Nang Dick Blumenthal Refuses to Call in Don Jr For Questioning Over Fake News Articles via\"\n",
      "---\n",
      "80 tweets of:\n",
      "\"It has been incorrectly reported that Rudy Giuliani and others will not be doing a counter to the Mueller Report That is Fake News Already 87 pages done but obviously cannot complete until we see the final Witch Hunt Report\"\n",
      "---\n",
      "75 tweets of:\n",
      "\"Does the Fake News Media ever mention the fact that Republicans with the very important help of my campaign Rallies WON THE UNITED STATES SENATE 53 to 47 All I hear is that the Open Border Dems won the House Senate alone approves judges &amp; others Big Republican Win\"\n",
      "---\n",
      "48 tweets of:\n",
      "\"42 On-Air Journalists Talk Working in the Age of Fake News\"\n",
      "---\n",
      "44 tweets of:\n",
      "\"Grassley Shuts Down Da Nang Dick Blumenthal Refuses to Call in Don Jr For Questioning Over Fake News Articles\"\n",
      "---\n",
      "40 tweets of:\n",
      "\"New Post Sale The True Story of Fake News How Mainstream Media Manipulates Millions\"\n",
      "---\n",
      "26 tweets of:\n",
      "\"EU Steps Up Fight Against Fake News Ahead of Elections\"\n",
      "---\n",
      "22 tweets of:\n",
      "\"Trump Tweeted Fake News A Journalist Responded With Story Of Slain Colleague\"\n",
      "---\n",
      "20 tweets of:\n",
      "\"Syrian Boy Waterboarded is FAKE NEWS via\"\n",
      "---\n",
      "20 tweets of:\n",
      "\"Fake News Two Altar Boys NOT Arrested For Putting Weed In The Lead Stories\"\n",
      "---\n",
      "18 tweets of:\n",
      "\"JUST IN Trump Calls Out Fake News Warns Of 87-Page Bombshell Report Countering Mueller via\"\n",
      "---\n",
      "18 tweets of:\n",
      "\"Ignore The Fake News Why Kolanovic Sees The S &amp; P Rising To 3 100 In 2019\"\n",
      "---\n",
      "17 tweets of:\n",
      "\"Liberal Fact-Checker Snopes Caught Approving Wildly Misleading Anti-GOP Fake News\"\n",
      "---\n",
      "17 tweets of:\n",
      "\"Trump Tweeted Fake News A Capital Gazette Journalist Responded With A Touching Story Of Slain Colleague\"\n",
      "---\n",
      "12 tweets of:\n",
      "\"Fake News Soviet-Style The Armenian Earthquake Rescue That Was Too Good To Be True\"\n",
      "---\n",
      "12 tweets of:\n",
      "\"If You Get 3 7 On This Quiz Youâ€™re Getting Sucker Punched By Fake News\"\n",
      "---\n",
      "12 tweets of:\n",
      "\"Flynn Jr Rips Fake News Media For Utter Lack of Integrity Covering His Father\"\n",
      "---\n",
      "12 tweets of:\n",
      "\"realDonaldTrump Does the Fake News Media ever mention the fact that Republicans with the very important help of my campaign Rallies WON THE UNITED STATES SENATE 53 to 47 All I hear is that the Open Border Dems won the House Senate alone approves judges &amp; others Big Re\"\n",
      "---\n",
      "11 tweets of:\n",
      "\"Good News About Fake News AI Learning to Detect It\"\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "### Frequency of title cased tweets\n",
    "# These are mostly article titles and bot-generated tweets\n",
    "\n",
    "# Reconstruct tokens into strings\n",
    "reconstructed_titles= []\n",
    "for tokens in plusavg_titlecase:\n",
    "    reconstructed_titles.append(' '.join(tokens))\n",
    "\n",
    "# Get frequencies and sort\n",
    "titles_sorted=nltk.FreqDist(reconstructed_titles).most_common()\n",
    "\n",
    "# And the winners are...\n",
    "for title,num in titles_sorted[0:20]:\n",
    "    print(f\"{num} tweets of:\\n\\\"{title}\\\"\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Frequency of ngrams in normalish title-cased tweets\n",
    "\n",
    "# First, casefold\n",
    "tokens_folded =[]\n",
    "for tokens in avg_titlecase:\n",
    "    folded=[]\n",
    "    for token in tokens: \n",
    "        folded.append(token.casefold())\n",
    "    tokens_folded.append(folded)\n",
    "    \n",
    "# Still have the same amount of tweets\n",
    "len(tokens_folded)==len(avg_titlecase)\n",
    "\n",
    "# Frequency of Trigrams\n",
    "folded_gramfreq=nltk.FreqDist()\n",
    "for tokens in tokens_folded:\n",
    "    folded_gramfreq.update(ngrams(tokens,3))\n",
    "\n",
    "# ANYthing other than fake news\n",
    "common_nonFN=[]\n",
    "for gram, freq in folded_gramfreq.most_common():\n",
    "    if \"fake\" not in gram and 'news' not in gram:\n",
    "        common_nonFN.append((gram,freq))\n",
    "        \n",
    "# Trump's \"ENEMY OF THE PEOPLE\" tweet was a v hot topic\n",
    "for gram,num in common_nonFN[0:25]:\n",
    "    print(gram,num)\n",
    "    \n",
    "print(\"\\n*** The End ***\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
