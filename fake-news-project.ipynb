{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fake-news-Dec-8th.json\", \"r\") as source:\n",
    "    alltweets = json.load(source)\n",
    "\n",
    "unique_tweets = set()\n",
    "for tweet in alltweets:\n",
    "    unique_tweets.update([tweet['full_text']])\n",
    "\n",
    "print(f\"Total tweets: {len(alltweets)}\")\n",
    "print(f\"Total unique (text): {len(unique_tweets)}\")\n",
    "\n",
    "ids =[]\n",
    "for tweet in alltweets:\n",
    "    ids.append(tweet['id'])  \n",
    "    \n",
    "ids.sort(reverse=True)\n",
    "first=ids[0]\n",
    "last=ids[-1]\n",
    "\n",
    "print(\"\\nTweets collected between:\")\n",
    "for tweet in alltweets:\n",
    "    if tweet['id'] == first:\n",
    "        print(tweet['created_at'])\n",
    "    if tweet['id'] == last: \n",
    "        print(tweet['created_at'])\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def tweet_tokenize(tweet: dict,*,casefold=False) -> List[str]:\n",
    "    pattern = \"\"\"(?x)                  # VERBOSE\n",
    "        (?:[A-Za-z]\\.)+                # abbreviations\n",
    "        | \\w+['â€™]\\w\\w?                 # contractions\n",
    "        | \\#\\w+                        # hashtags\n",
    "        | \\@\\w+                        # mentions\n",
    "        | https?://\\w+\\.\\w+\\.?(?:/\\w+) # links\n",
    "        | \\w+(?:-\\w+)*                 # hyphenated words/names\n",
    "        | \\$?\\d+(?:\\.\\d+)?%?           # currency or percentages\n",
    "        | &amp;                        # &\n",
    "        \"\"\"\n",
    "    if casefold: text = tweet['full_text'].casefold()\n",
    "    else: text = tweet['full_text']\n",
    "    return nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "def ngrams(symbols: list, n=3):\n",
    "    if len(symbols) < n:\n",
    "        return\n",
    "    prev_context = symbols[:n - 1]\n",
    "    for i in range(len(symbols) - n + 1):\n",
    "        yield tuple(symbols[i:i + n])\n",
    "        \n",
    "def has_hash(tweets: List[dict]) -> List[dict]:\n",
    "    # Yields all tweets that contain hashtag(s)\n",
    "    for tweet in tweets:\n",
    "        if tweet['entities']['hashtags']: yield tweet\n",
    "            \n",
    "def has_url(tweets: List[dict]) -> List[dict]:\n",
    "    # Yields all tweets that contain URL(s)\n",
    "    # Urls indicate quoted tweets link/media sharing, etc.\n",
    "    for tweet in tweets:\n",
    "        if tweet['entities']['urls']: yield tweet\n",
    "\n",
    "def has_quote(tweets: List[dict]) -> List[dict]:\n",
    "    # Yields tweets that quote another tweet\n",
    "    for tweet in tweets:\n",
    "        if tweet['is_quote_status']: yield tweet\n",
    "            \n",
    "def yield_quoted(tweets: List[dict]) -> dict:\n",
    "    # Yields tweets quoted by another user\n",
    "    # Identifies tweets marked as quoting but don't contain quoted content\n",
    "    for tweet in has_quote(tweets):\n",
    "        try: yield tweet['quoted_status']\n",
    "        except KeyError: print(f\"id: {tweet['id']} has no quoted tweet.\")\n",
    "\n",
    "def get_quoted(tweet):\n",
    "    return tweet['quoted_status']\n",
    "\n",
    "def tweet_hashtags(tweet: dict,*,casefold=True) -> str:\n",
    "    # Yields hashtag(s) of a tweet\n",
    "    taglst = tweet['entities']['hashtags']\n",
    "    for tag in taglst:\n",
    "        if casefold: yield tag['text'].casefold()\n",
    "        else: yield tag['text']\n",
    "\n",
    "def tweet_urls(tweet: dict,*,casefold=False) -> str:\n",
    "    # Yields non-status URL(s) embedded in a given tweet\n",
    "    # some urls are case-sensitive\n",
    "    statusfilter='twitter.com/\\w+?/status/'\n",
    "    in_urlst = tweet[\"entities\"][\"urls\"]\n",
    "    out_urlst = []\n",
    "    for url_dict in in_urlst:\n",
    "        if not re.findall(statusfilter, url_dict['expanded_url']):\n",
    "            out_urlst.append(url_dict)\n",
    "    for url in out_urlst:\n",
    "        if casefold: yield url['expanded_url'].casefold()\n",
    "        else: yield url['expanded_url']\n",
    "    \n",
    "def get_screenname(tweet: dict,*,casefold=True) -> str:\n",
    "    if casefold: return tweet['user']['screen_name'].casefold()\n",
    "    else: return tweet['user']['screen_name']\n",
    "\n",
    "def search_keys(target_query: str,target_dict: dict) -> str:\n",
    "    # Yield keys to access tweets by hashtag, url or quoted user\n",
    "    for key in target_dict.keys():\n",
    "        if target_query in key: yield key   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casefold and tokenize all tweets, and get rid of urls\n",
    "tokenized_tweets = []\n",
    "for tweet in alltweets:\n",
    "    tokens = []\n",
    "    for token in tweet_tokenize(tweet,casefold=True):\n",
    "        if not token.startswith('http'): tokens.append(token)\n",
    "    tokenized_tweets.append(tokens)\n",
    "\n",
    "# Calculate avg length of tweets\n",
    "all_lens=0\n",
    "for tweet in tokenized_tweets:\n",
    "    all_lens+=len(tweet)\n",
    "avg_len = all_lens/len(tokenized_tweets)\n",
    "print(f\"Then avergage tweet length (without urls) is *{round(avg_len,2)}* tokens.\")\n",
    "\n",
    "# Complie all short tweets that begin or end with 'fake news'\n",
    "shortnsweet = []\n",
    "for tweet in tokenized_tweets:\n",
    "    if len(tweet) <= 10 \\\n",
    "    and (tweet[0:2]==['fake','news'] or tweet[-2:]==['fake','news']):\n",
    "        shortnsweet.append(tweet)\n",
    "print(f\"*{len(shortnsweet)}* tweets are <= 10 chars and begin or end with 'fake news'.\")  \n",
    "\n",
    "# Find tweets that are at least 30% in ALL CAPS\n",
    "yelling = []\n",
    "for tweet in alltweets:\n",
    "    tokens = tweet_tokenize(tweet,casefold=False)\n",
    "    yells=0\n",
    "    for token in tokens:\n",
    "        if token.isupper(): yells+=1\n",
    "    if yells/len(tokens) >= 0.3: yelling.append(tweet)\n",
    "        \n",
    "print(f\"*{len(yelling)}* involve some yelling.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all tweets with hashtags\n",
    "has_tags = []\n",
    "for tweet in has_hash(alltweets):\n",
    "    has_tags.append(tweet)\n",
    "print(f\"*{len(has_tags)}* tweets contain hashtags.\")\n",
    "    \n",
    "# Collect all tweets with urls\n",
    "# Urls indicate quoted tweets *and* links\n",
    "has_urls=[]\n",
    "for tweet in has_url(alltweets):\n",
    "    has_urls.append(tweet)\n",
    "print(f\"*{len(has_urls)}* tweets contain urls.\")\n",
    "\n",
    "# Collect only the tweets marked as quoting another user\n",
    "are_quoting=[]\n",
    "for tweet in has_quote(alltweets):\n",
    "    are_quoting.append(tweet)\n",
    "print(f\">> *{len(are_quoting)}* tweets are marked as quoting another user.\")\n",
    "\n",
    "# Collect all other tweets that shared a link\n",
    "not_quoting=[]\n",
    "for tweet in has_urls:\n",
    "    if tweet not in are_quoting:\n",
    "        not_quoting.append(tweet)\n",
    "print(f\">> *{len(not_quoting)}* tweets shared at least one link.\")\n",
    "\n",
    "# Collect the embedded quoted tweets\n",
    "is_quoted_tweet=[]\n",
    "for tweet in yield_quoted(alltweets):\n",
    "    is_quoted_tweet.append(tweet)\n",
    "print(f\">> *{len(is_quoted_tweet)}* quoted tweets are accesible via the API.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up tweets that are quote-marked w/o an embedded tweet\n",
    "bad = {}\n",
    "for tweet in are_quoting:\n",
    "    if tweet['id'] in bad:\n",
    "        tweet['is_quote_status'] = False\n",
    "\n",
    "are_quoting=[]\n",
    "for tweet in has_quote(alltweets):\n",
    "    are_quoting.append(tweet)\n",
    "print(f\">> *{len(are_quoting)}* tweets are now marked as quoting another user.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect 'quoted' tweet entities/users for comparison\n",
    "quoted_users = set()\n",
    "for tweet in are_quoting:\n",
    "    quoted_users.update([get_screenname(get_quoted(tweet))])\n",
    "    \n",
    "# Map tweets by the quoted entity:\n",
    "by_quoted_user = nltk.defaultdict(list)\n",
    "for tweet in are_quoting:\n",
    "    q = get_quoted(tweet)\n",
    "    by_quoted_user[get_screenname(q)].append(tweet)\n",
    "\n",
    "# Make sure all quoted users are accounted for\n",
    "assert len(quoted_users) == len(by_quoted_user)\n",
    "\n",
    "# Calculate the number of tweets quoting each entity\n",
    "tweets_per_quoted=[]\n",
    "for key in by_quoted_user.keys():\n",
    "    tweets_per_quoted.append((key, len(by_quoted_user[key])))\n",
    "    \n",
    "tweets_per_quoted.sort(key=lambda x: x[1], reverse=True)\n",
    "top_20_quoted = tweets_per_quoted[0:20]\n",
    "\n",
    "print(\"Top 20 quoted users:\")\n",
    "for ent, num in top_20_quoted:\n",
    "    print('---')\n",
    "    print(f\"{num} tweets quoted @{ent}.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map hashtags associated with the quoted entities\n",
    "tags_with_quoted = nltk.defaultdict(set)\n",
    "for key in by_quoted_user.keys():\n",
    "    for tweet in has_hash(by_quoted_user[key]):\n",
    "        tags_with_quoted[key].update(tweet_hashtags(tweet))\n",
    "\n",
    "print(\"Tags associated with @civmilair:\")\n",
    "print(\"#\"+\"\\n#\".join(tags_with_quoted['civmilair']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequecy of users among all tweets\n",
    "most_active = nltk.FreqDist()\n",
    "for tweet in alltweets:\n",
    "    most_active.update([get_screenname(tweet)])\n",
    "\n",
    "allusers_sorted=most_active.most_common()\n",
    "\n",
    "# These are mostly bots\n",
    "for ent, num in allusers_sorted[0:20]:\n",
    "    print(f\"@{ent} tweeted {num} times.\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of times each tag appears\n",
    "tagfreq = nltk.FreqDist()\n",
    "for tweet in has_hash(alltweets):\n",
    "    tagfreq.update(tweet_hashtags(tweet))\n",
    "    \n",
    "alltags_sorted=tagfreq.most_common()\n",
    "\n",
    "# Map tweets by the hashtags they contain\n",
    "tweets_by_tag = nltk.defaultdict(list)\n",
    "for tweet in has_hash(alltweets):\n",
    "    for tag in tweet_hashtags(tweet):\n",
    "        tweets_by_tag[tag].append(tweet)\n",
    "        \n",
    "assert len(alltags_sorted) == len(tweets_by_tag)\n",
    "\n",
    "for tag, num in alltags_sorted[0:20]:\n",
    "    print(f\"#{tag} was tweeted {num} times.\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Strip urls as much as possible to map tweets by their news source\n",
    "all_links = []\n",
    "for tweet in not_quoting:\n",
    "    for tweeturl in tweet_urls(tweet):\n",
    "        all_links.append(tweeturl)\n",
    "        \n",
    "url_stripped=\"\"\"(?x) \n",
    "            (?:https?://)?          # http or https protocol (non-capturing)\n",
    "            (                       # begin capture match.groups()[0]\n",
    "            (www\\.\\w+(-?\\.?\\w+)?)?  # www + host name (optional)\n",
    "            (\\w+-)?(\\w+)?           # cont'd host name with hyphen  \n",
    "            (\\.\\w+(-?\\.?\\w+)*)?     # cont'd host names w/ hypen or dot                 \n",
    "            )                       # end capture match.groups()[0]\n",
    "            \"\"\"\n",
    "\n",
    "sources = dict()\n",
    "stripped_urls = set()\n",
    "\n",
    "for url in all_links:\n",
    "    # Map stripped urls by the full url in the tweet\n",
    "    sources[url]=re.search(url_stripped, url).groups()[0]\n",
    "    # Also collect stripped urls for comparison\n",
    "    stripped_urls.update([re.search(url_stripped, url).groups()[0]])\n",
    "\n",
    "## Map tweets by stripped url\n",
    "tweets_by_source=nltk.defaultdict(list)\n",
    "for tweet in not_quoting:\n",
    "    for url in tweet_urls(tweet):\n",
    "        tweets_by_source[sources[url]].append(tweet)\n",
    "\n",
    "assert len(stripped_urls) == len(tweets_by_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of tweets containing each source (e.g. 'www.cnn.com')\n",
    "tweets_per_source = []\n",
    "for key in tweets_by_source.keys():\n",
    "    tweets_per_source.append((key,len(tweets_by_source[key])))\n",
    "    \n",
    "tweets_per_source.sort(key=lambda x: x[1], reverse=True)\n",
    "top_20_sources = tweets_per_source[0:20]\n",
    "\n",
    "# Map tags associtated with each source\n",
    "associated_tags = nltk.defaultdict(set)\n",
    "for key in tweets_by_source.keys():\n",
    "    for tweet in has_hash(tweets_by_source[key]): \n",
    "        associated_tags[key].update(tweet_hashtags(tweet))\n",
    "        \n",
    "for source,num in top_20_sources:\n",
    "    print('---')\n",
    "    print(f\"{num} shared from {source}\")\n",
    "print('---')\n",
    "print('---')\n",
    "print('GatewayPundit associated tags: ')\n",
    "print('#'+', #'.join(associated_tags['www.thegatewaypundit.com']))\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to5=[]\n",
    "to10=[] \n",
    "to15=[] \n",
    "to20=[]\n",
    "to30=[] \n",
    "to40=[]\n",
    "to50=[] \n",
    "more50=[]\n",
    "divided_by_size = [to5, to10, to15, to20, to30, to40, to50, more50]\n",
    "\n",
    "tokenized = (tweet_tokenize(tweet,casefold=False) for tweet in alltweets)\n",
    "for tweet in tokenized:\n",
    "    tokens = []\n",
    "    for token in tweet:\n",
    "        if not token.startswith('http') \\\n",
    "        and not token.startswith('@') \\\n",
    "        and not token.startswith('#'): tokens.append(token)\n",
    "    if len(tokens) <= 5: to5.append(tokens)\n",
    "    elif len(tokens) <= 10: to10.append(tokens)\n",
    "    elif len(tokens) <= 15: to15.append(tokens)\n",
    "    elif len(tokens) <= 20: to20.append(tokens)\n",
    "    elif len(tokens) <= 30: to30.append(tokens)\n",
    "    elif len(tokens) <= 40: to40.append(tokens)\n",
    "    elif len(tokens) <= 50: to50.append(tokens)\n",
    "    else: more50.append(tokens)\n",
    "\n",
    "def count_titles(tokens):\n",
    "    total = 0\n",
    "    for token in tokens:\n",
    "        if token.istitle(): total+=1\n",
    "    return total\n",
    "\n",
    "individual_avgs = []\n",
    "for i in range(len(divided_by_size)):\n",
    "    t_avg = []\n",
    "    for tokens in divided_by_size[i]:\n",
    "        titles=count_titles(tokens)\n",
    "        avg_title=round(titles/len(tokens)*100)\n",
    "        t_avg.append(avg_title)\n",
    "    individual_avgs.append(t_avg)\n",
    "\n",
    "ovall_avgs=[]\n",
    "for i in range(len(individual_avgs)):\n",
    "    all_avgs = 0\n",
    "    for avg in individual_avgs[i]: all_avgs+=avg\n",
    "    ovall_avgs.append(all_avgs/len(individual_avgs[i]))\n",
    "    \n",
    "plusavg_titlecase = []\n",
    "avg_titlecase = []\n",
    "for i in range(8):\n",
    "    for index in range(len(divided_by_size[i])):\n",
    "        if individual_avgs[i][index] > round(ovall_avgs[i],1)*1.5: \n",
    "            plusavg_titlecase.append(divided_by_size[i][index])\n",
    "        else: \n",
    "            avg_titlecase.append(divided_by_size[i][index])\n",
    "            \n",
    "print(f\"{len(to5)} tweets under 5 tokens avg {round(ovall_avgs[0],1)}% title case.\")\n",
    "print(f\"{len(to10)} tweets w/ 6-10 tokens avg {round(ovall_avgs[1],1)}% title case.\")\n",
    "print(f\"{len(to15)} tweets w/ 11-15 tokens avg {round(ovall_avgs[2],1)}% title case.\")\n",
    "print(f\"{len(to20)} tweets w/ 16-20 tokens avg {round(ovall_avgs[3],1)}% title case.\")\n",
    "print(f\"{len(to30)} tweets w/ 21-30 tokens avg {round(ovall_avgs[4],1)}% title case.\")\n",
    "print(f\"{len(to40)} tweets w/ 31-40 tokens avg {round(ovall_avgs[5],1)}% title case.\")\n",
    "print(f\"{len(to50)} tweets w/ 41-50 tokens avg {round(ovall_avgs[6],1)}% title case.\")\n",
    "print(f\"{len(more50)} tweets over 50 tokens avg {round(ovall_avgs[7],1)}% title case.\")\n",
    "print('---')\n",
    "print(f\"{len(plusavg_titlecase)} tweets are above average for their group.\")\n",
    "print(f\"{len(avg_titlecase)} tweets are about at or below average.\")\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Frequency of title cased tweets\n",
    "# These are mostly article titles and bot-generated tweets\n",
    "\n",
    "# Reconstruct tokens into strings\n",
    "reconstructed_titles= []\n",
    "for tokens in plusavg_titlecase:\n",
    "    reconstructed_titles.append(' '.join(tokens))\n",
    "\n",
    "# Get frequencies and sort\n",
    "titles_sorted=nltk.FreqDist(reconstructed_titles).most_common()\n",
    "\n",
    "# And the winners are...\n",
    "for title,num in titles_sorted[0:20]:\n",
    "    print(f\"{num} tweets of:\\n\\\"{title}\\\"\")\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Frequency of ngrams in normalish title-cased tweets\n",
    "\n",
    "# First, casefold\n",
    "tokens_folded =[]\n",
    "for tokens in avg_titlecase:\n",
    "    folded=[]\n",
    "    for token in tokens: \n",
    "        folded.append(token.casefold())\n",
    "    tokens_folded.append(folded)\n",
    "    \n",
    "# Still have the same amount of tweets\n",
    "len(tokens_folded)==len(avg_titlecase)\n",
    "\n",
    "# Frequency of Trigrams\n",
    "folded_gramfreq=nltk.FreqDist()\n",
    "for tokens in tokens_folded:\n",
    "    folded_gramfreq.update(ngrams(tokens,3))\n",
    "\n",
    "# ANYthing other than fake news\n",
    "common_nonFN=[]\n",
    "for gram, freq in folded_gramfreq.most_common():\n",
    "    if \"fake\" not in gram and 'news' not in gram:\n",
    "        common_nonFN.append((gram,freq))\n",
    "        \n",
    "# Trump's \"ENEMY OF THE PEOPLE\" tweet was a v hot topic\n",
    "for gram,num in common_nonFN[0:25]:\n",
    "    print(gram,num)\n",
    "    \n",
    "print(\"\\n*** The End ***\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
