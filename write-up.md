# methods-1-final

Wikipedia primarily defines fake news as a type of “yellow journalism or propaganda that consists of deliberate disinformation or hoaxes spread via traditional print and broadcast news media or online social media.” Identifiable instances of these hoaxes date back as far as the 13th Century BC and have been ongoing ever since. Today’s observable growth in usage of the term ‘fake news’, however, can largely be attributed to the events occurring up to and after the 2016 American Presidential Election. Arguably, the most influential individual in the term’s expansion is, of course, none other than Donald Trump. In one of his most widely cited tweets, Trump appeared to conflate the words ‘negative’ and ‘Fake’ when referring to press specifically about him:

@realDonaldTrump (on May 9, 2018):

The Fake News is working overtime. Just reported that, despite the tremendous success we are having with the economy & all things else, 91% of the Network News about me is negative (Fake). Why do we work so hard in working with the media when it is corrupt? Take away credentials?

This particular usage has widely taken hold among both supporters and critics of Trump—resulting in many discussions and debates about the diluted and/or misleading nature of the term’s current-day connotations. Because of this, government legislators and other organizations have made attempts to stop internal usage of the term and opt for terms like ‘false news’ or ‘mis-/dis-information,’ even going so far as to ban the term altogether through legislation or policy. Clearly ‘fake news’ has permeated our culture on both the macro and the micro level. With such a controversial and influential topic, there is bound to be a wealth of data ripe for the picking.

Personally, I first took interest in this ‘conversational’ treatment when a (fully anti-Trump) friend used it to refer to something he didn’t like in an Instagram group chat. From there, I searched Twitter and found a lot of very strange and often dark-looking tweets, all using the term. Going into it somewhat blind, I was able to use a relatively newer Python package called TwitterAPI to scrape over 15,000 tweets in one session. Unfortunately, around 1,000 of the Tweets did not actually contain the term within the text (presumably it was somewhere else in the metadata, e.g., an embedded tweet, profile text, screenname, etc.). To handle this in IPython, I used a simple regular expression, à la fake.*?news (ignoring case), to find only the tweets containing it in the full_text and ended up with 14406 tweets, 14000 of which were unique in their textual content out of the box. 
After spending some time reviewing the tweets (and the huge amount of metadata they contain), I started making some tools to begin basic sorting and analysis of the data for text content and entities like hashtags, URLs, and user engagements. Initially, I had focused a quite a few efforts on trying to extract information about the users’ political affiliations either through their textual content or their profile description, but in my best efforts, I could only account for roughly 2,500 tweets, likely with quite a few errors. 
Taking a different approach, I decided to try making the data more searchable from a research perspective. Already, I had been doing what I could to keep the metadata intact in case anything should prove particularly useful. In this way, I was able to map lists of tweets into tables where they could be accessible by either the user/entity that was ‘quoted’, a hashtag that was used, or a pared down representation of the shared link source (e.g., “www.cnn.com” instead of “https://www.cnn.com/2018/12/08/politics/trump-france-protests/index.html”). The  source mapped dictionary took the most effort by far, but the results were mostly positive. Here are the steps I took: 

1.	Collected all of the links and compiled the short URLs (bitl.ly, buff.ly, etc.) to a separate list and wrote to a json file.
2.	Used the requests package to retrieve the expanded URLs (a rather slow process) and set up the function to return a tuple of both short and long URLs. (This was done using url_extend.py)
3.	Found the tweet URLs that match the short URL, and overwrote w/ the expanded URL in the API. (this was done using url_swap.py)
4.	Then I stripped down the URLs just enough to identify the source using the re package (most of them could’ve been identified by the pattern //.*?/, but a several elusive ones made it more difficult).
5.	Finally, I identified the tweets by matching the long URLs and mapping them to the pared down key.

All in all, these separate mappings appeared to prove useful for further, more focused analysis because they allow tweets to be accessed by shared features. This method allowed me to get more dimensional and/or generalized frequency distributions—especially for the source-mapped data—while keeping the APIs intact. Namely, these tables allowed for associative distributions where a source URL, Twitter user or hashtag can be paired with lists of hashtags, other users, common tokens and the like.
Finally, I didn’t want to complete the project without doing some kind of analysis of the actual text. Noticing that title casing had plentiful distribution, I focused on identifying tweets that had above average amounts of title casing. In dividing the tweets by token size and calculating the average title casings for each group, I hoped to reduce the Article Title Sharing And Bot-Generated Noise (there were a lot of bots) for more commentary-related n-grams. The latter portion was perhaps not so successful, but the former actually allowed me to identify quite a few titles of some of the most shared articles (which ended up being fairly intriguing). I definitely think there’s a lot more possibility here, and I’m sure I’ll be revisiting this project as my Python skills continue to grow.
